# Gi·∫£i Th√≠ch Chi Ti·∫øt: Balanced Error v√† Plugin (Balance Mode & Worst Mode)

## üìã M·ª•c L·ª•c
1. [Balanced Error - C√¥ng Th·ª©c v√† C√°ch T√≠nh](#1-balanced-error---c√¥ng-th·ª©c-v√†-c√°ch-t√≠nh)
2. [Code T√≠nh Balanced Error](#2-code-t√≠nh-balanced-error)
3. [Plugin - Balance Mode (Algorithm 1)](#3-plugin---balance-mode-algorithm-1)
4. [Plugin - Worst Mode (Algorithm 2)](#4-plugin---worst-mode-algorithm-2)
5. [So S√°nh Hai Modes](#5-so-s√°nh-hai-modes)

---

## 1. Balanced Error - C√¥ng Th·ª©c v√† C√°ch T√≠nh

### 1.1. ƒê·ªãnh Nghƒ©a t·ª´ Paper

Theo paper "Learning to Reject Meets Long-Tail Learning" (ICLR 2024), **Balanced Error** ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a nh∆∞ sau:

Cho ph√¢n lo·∫°i v·ªõi **Learning to Reject (L2R)**:

```
R^rej_bal(h, r) = (1/K) * Œ£_k P(y ‚â† h(x) | r(x) = 0, y ‚àà G_k) + c ¬∑ P(r(x) = 1)
```

Trong ƒë√≥:
- `K`: S·ªë l∆∞·ª£ng groups (v√≠ d·ª•: K=2 cho head/tail)
- `G_k`: Group th·ª© k (v√≠ d·ª•: G_0=head, G_1=tail)
- `h(x)`: Classifier prediction
- `r(x)`: Rejector (0=accept, 1=reject)
- `c`: Rejection cost

**Ph·∫ßn ch√≠nh c·ªßa balanced error** (kh√¥ng t√≠nh cost term):
```
Balanced Error = (1/K) * Œ£_k P(y ‚â† h(x) | r(x) = 0, y ‚àà G_k)
```

ƒê√¢y l√† **trung b√¨nh c·ªßa c√°c conditional error rates** tr√™n t·ª´ng group, ch·ªâ t√≠nh tr√™n c√°c samples **ƒë∆∞·ª£c accept** (kh√¥ng reject).

### 1.2. √ù Nghƒ©a

**T·∫°i sao g·ªçi l√† "Balanced"?**
- Kh√¥ng gi·ªëng standard accuracy (b·ªã ·∫£nh h∆∞·ªüng b·ªüi class imbalance)
- Balanced error **ƒë·ªëi x·ª≠ c√¥ng b·∫±ng** v·ªõi m·ªói group: m·ªói group c√≥ weight = 1/K
- V√≠ d·ª• v·ªõi 2 groups: 
  - Head error: 10%
  - Tail error: 30%
  - **Balanced error = (10% + 30%) / 2 = 20%**

**T·∫°i sao ch·ªâ t√≠nh tr√™n accepted samples?**
- Ch·ªâ quan t√¢m ƒë·∫øn **ch·∫•t l∆∞·ª£ng d·ª± ƒëo√°n** tr√™n c√°c samples m√† model t·ª± tin (accept)
- Rejected samples kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t√≠nh error

---

## 2. Code T√≠nh Balanced Error

### 2.1. V·ªã Tr√≠ Code Ch√≠nh

Code t√≠nh balanced error n·∫±m ·ªü nhi·ªÅu n∆°i, nh∆∞ng **implementation ch√≠nh** c√≥ trong:

1. **`run_balanced_plugin_gating.py`** - d√≤ng 434-504: H√†m `compute_metrics()`
2. **`src/models/ltr_plugin.py`** - d√≤ng 309-399: H√†m `compute_selective_metrics()`
3. **`run_worst_plugin_gating.py`** - d√≤ng 329-383: H√†m `compute_metrics()`

### 2.2. Chi Ti·∫øt Implementation

D∆∞·ªõi ƒë√¢y l√† code t·ª´ `run_balanced_plugin_gating.py` (d√≤ng 434-504):

```python
@torch.no_grad()
def compute_metrics(
    preds: torch.Tensor,
    labels: torch.Tensor,
    reject: torch.Tensor,
    class_to_group: torch.Tensor,
    class_weights: Optional[torch.Tensor] = None,
) -> Dict[str, float]:
    # B∆∞·ªõc 1: T√°ch accepted/rejected samples
    accept = ~reject
    if accept.sum() == 0:
        # N·∫øu reject h·∫øt, return worst case
        return {
            "balanced_error": 1.0,
            "worst_group_error": 1.0,
            ...
        }
    
    # Ch·ªâ l·∫•y accepted samples
    preds_a = preds[accept]
    labels_a = labels[accept]
    errors = (preds_a != labels_a).float()  # [N_accept]
    
    # B∆∞·ªõc 2: T√≠nh error cho t·ª´ng group
    groups = class_to_group[labels_a]  # Group c·ªßa m·ªói accepted sample
    num_groups = int(class_to_group.max().item() + 1)
    
    group_errors = []
    
    # T√≠nh conditional error rate cho t·ª´ng group
    for g in range(num_groups):
        mask = groups == g  # Samples thu·ªôc group g
        
        if mask.sum() == 0:
            # Kh√¥ng c√≥ accepted samples t·ª´ group n√†y
            group_errors.append(1.0)  # Worst case
        else:
            # P(y ‚â† h(x) | r(x) = 0, y ‚àà G_k)
            # = s·ªë l·ªói trong group g / t·ªïng samples trong group g (accepted)
            num_errors_in_group = errors[mask].sum().item()
            num_accepted_in_group = mask.sum().item()
            conditional_error = num_errors_in_group / num_accepted_in_group
            group_errors.append(conditional_error)
    
    # B∆∞·ªõc 3: Balanced Error = trung b√¨nh c·ªßa group errors
    balanced_error = float(np.mean(group_errors))
    
    # Worst-group Error = max c·ªßa group errors
    worst_group_error = float(np.max(group_errors))
    
    return {
        "balanced_error": balanced_error,
        "worst_group_error": worst_group_error,
        "group_errors": group_errors,
        ...
    }
```

### 2.3. V√≠ D·ª• C·ª• Th·ªÉ

Gi·∫£ s·ª≠ c√≥:
- **Head group (G_0)**: 1000 accepted samples, 100 errors ‚Üí error = 10%
- **Tail group (G_1)**: 500 accepted samples, 150 errors ‚Üí error = 30%

**Balanced Error** = (10% + 30%) / 2 = **20%**

**Worst-group Error** = max(10%, 30%) = **30%**

---

## 3. Plugin - Balance Mode (Algorithm 1)

### 3.1. M·ª•c Ti√™u

Optimize **Balanced Error** v·ªõi constraint v·ªÅ rejection rate:

```
Minimize: R^rej_bal(h, r) = (1/K) * Œ£_k P(y ‚â† h(x) | r(x) = 0, y ‚àà G_k) + c ¬∑ P(r(x) = 1)
```

### 3.2. Decision Rules (Theorem 1)

Theo paper, Bayes-optimal classifier v√† rejector c√≥ d·∫°ng:

**Classifier:**
```
h*(x) = argmax_y (1/Œ±[y]) ¬∑ Œ∑_y(x)
```

**Rejector:**
```
r*(x) = 1  n·∫øu  max_y(1/Œ±[y]¬∑Œ∑_y(x)) < Œ£_y'(1/Œ±[y'] - Œº[y'])¬∑Œ∑_y'(x) - c
```

Trong ƒë√≥:
- `Œ∑_y(x)`: Mixture posterior probability c·ªßa class y (t·ª´ gating network)
- `Œ±[y]`: Group-level reweighting parameter (Œ±_head, Œ±_tail)
- `Œº[y]`: Group-level threshold adjustment parameter
- `c`: Rejection cost

### 3.3. Algorithm 1: Power Iteration

**Code location**: `src/models/ltr_plugin.py` - class `LtRPowerIterOptimizer` (d√≤ng 420-644)

**Thu·∫≠t to√°n:**

```
For m·ªói Œº trong grid search:
    Œ±^(0) ‚Üê kh·ªüi t·∫°o (d·ª±a tr√™n class priors)
    
    For m = 0 to M-1:  # Power iteration
        # B∆∞·ªõc 1: Construct classifier v√† rejector v·ªõi Œ±^(m)
        h^(m+1)(x) = argmax_y (1/Œ±^(m)[y]) ¬∑ p_y(x)
        r^(m+1)(x) = 1 n·∫øu max_y(...) < threshold
        
        # B∆∞·ªõc 2: Update Œ± d·ª±a tr√™n empirical coverage
        Œ±^(m+1)_k = K * P(y ‚àà G_k, r^(m+1)(x) = 0)
        # T·ª©c l√†: Œ±_k = K * (t·ª∑ l·ªá samples t·ª´ group k ƒë∆∞·ª£c accept)
    
    # B∆∞·ªõc 3: Evaluate objective v·ªõi (h^(M), r^(M))
    objective = balanced_error + c * (1 - coverage)
    
# Tr·∫£ v·ªÅ best (Œ±, Œº, c) c√≥ objective th·∫•p nh·∫•t
```

### 3.4. Chi Ti·∫øt Code Implementation

#### 3.4.1. Initialize Alpha

```python
def _initialize_alpha(self, labels, class_to_group, sample_weights):
    """Kh·ªüi t·∫°o Œ±^(0) d·ª±a tr√™n class priors."""
    num_groups = class_to_group.max().item() + 1
    alpha = np.zeros(num_groups)
    
    for g in range(num_groups):
        # T√≠nh t·ª∑ l·ªá samples thu·ªôc group g
        group_mask = class_to_group[labels] == g
        proportion = group_mask.sum().float().item() / len(labels)
        
        # Œ±_k = K * proportion (ƒë·ªÉ ƒë·∫£m b·∫£o Œ± ‚àà (0, K))
        alpha[g] = num_groups * proportion
    
    return alpha
```

#### 3.4.2. Update Alpha from Coverage

```python
def _update_alpha_from_coverage(self, reject, labels, class_to_group):
    """Update Œ± d·ª±a tr√™n empirical coverage."""
    num_groups = class_to_group.max().item() + 1
    alpha = np.zeros(num_groups)
    accept = ~reject
    N = len(labels)
    
    for g in range(num_groups):
        # T√¨m samples t·ª´ group g
        in_group = class_to_group[labels] == g
        
        # T√¨m samples t·ª´ group g ƒë∆∞·ª£c accept
        accepted_in_group = accept & in_group
        
        # Œ±_k^(m+1) = K * P(y ‚àà G_k, r(x) = 0)
        # = K * (s·ªë samples accepted t·ª´ group g / t·ªïng samples)
        empirical_coverage = accepted_in_group.sum().float().item() / N
        alpha[g] = num_groups * empirical_coverage
    
    return alpha
```

#### 3.4.3. Power Iteration Loop

```python
def search(self, plugin, mixture_posterior, labels, ...):
    """Power iteration ƒë·ªÉ t√¨m optimal (Œ±, Œº, c)."""
    
    # Grid search over Œº v√† c
    for mu, cost in search_grid:
        # Kh·ªüi t·∫°o Œ±
        alpha = self._initialize_alpha(labels, class_to_group)
        
        # Power iteration
        for m in range(self.num_iters):
            # Set parameters
            plugin.set_parameters(alpha=alpha, mu=mu, cost=cost)
            
            # Construct (h, r)
            predictions = plugin.predict_class(mixture_posterior)
            reject = plugin.predict_reject(mixture_posterior)
            
            # Update Œ±
            alpha_new = self._update_alpha_from_coverage(
                reject, labels, class_to_group
            )
            
            # Damping ƒë·ªÉ ·ªïn ƒë·ªãnh
            alpha = (1 - damping) * alpha + damping * alpha_new
            
            # Check convergence
            if np.abs(alpha_new - alpha).max() < 1e-4:
                break
        
        # Evaluate objective
        metrics = compute_selective_metrics(...)
        objective = metrics['balanced_error'] + cost * (1 - metrics['coverage'])
        
        # Track best
        if objective < best_objective:
            best_result = (alpha, mu, cost)
    
    return best_result
```

### 3.5. V√≠ D·ª• Workflow

**Input:**
- Mixture posterior t·ª´ gating network: `Œ∑(x) = [0.6, 0.3, 0.1]` (3 classes)
- Labels: `y = 0`
- Group mapping: `[0=head, 1=head, 2=tail]`

**Initialization:**
- `Œ±^(0) = [1.5, 0.5]` (head c√≥ nhi·ªÅu samples h∆°n tail)

**Iteration 1:**
- Classifier: `h(x) = argmax_y (1/Œ±[y]) * Œ∑_y = argmax([0.4, 0.2, 2.0]) = 2`
- Rejector: t√≠nh threshold v√† quy·∫øt ƒë·ªãnh reject hay kh√¥ng
- Update Œ± d·ª±a tr√™n coverage th·ª±c t·∫ø

**Convergence:**
- Œ± h·ªôi t·ª• v·ªÅ `[1.2, 0.8]` (tail ƒë∆∞·ª£c up-weight ƒë·ªÉ balance error)

---

## 4. Plugin - Worst Mode (Algorithm 2)

### 4.1. M·ª•c Ti√™u

Optimize **Worst-group Error** (minimize maximum error across groups):

```
Minimize: R^rej_wst(h, r) = max_k P(y ‚â† h(x) | r(x) = 0, y ‚àà G_k) + c ¬∑ P(r(x) = 1)
```

### 4.2. Algorithm 2: Exponentiated Gradient

**Code location**: `src/models/ltr_plugin.py` - class `LtRWorstGroupOptimizer` (d√≤ng 1065-1278)

**Thu·∫≠t to√°n:**

```
Œ≤^(0) ‚Üê uniform (1/K cho m·ªói group)

For t = 0 to T-1:
    # B∆∞·ªõc 1: Solve cost-sensitive problem v·ªõi Œ≤^(t)
    # G·ªçi Algorithm 1 v·ªõi weighted objective: Œ£_k Œ≤_k * e_k
    (h^(t), r^(t)) ‚Üê Algorithm1(Œ≤^(t), c)
    
    # B∆∞·ªõc 2: Compute group errors tr√™n validation set
    e_k^(t) ‚Üê P(y ‚â† h^(t)(x) | r^(t)(x) = 0, y ‚àà G_k)
    
    # B∆∞·ªõc 3: Update Œ≤ b·∫±ng exponentiated gradient
    Œ≤^(t+1)_k ‚àù Œ≤^(t)_k * exp(Œæ * e_k^(t))
    Œ≤^(t+1) ‚Üê normalize v·ªÅ simplex

# Tr·∫£ v·ªÅ (h, r) c√≥ worst-group error th·∫•p nh·∫•t
```

### 4.3. Chi Ti·∫øt Code Implementation

#### 4.3.1. Generalized Plugin v·ªõi Œ≤

Trong worst mode, plugin s·ª≠ d·ª•ng **Œ≤ weights** ƒë·ªÉ up-weight groups c√≥ error cao:

```python
class GeneralizedLtRPlugin(nn.Module):
    """Plugin v·ªõi Œ≤ weights cho worst-group optimization."""
    
    def _u_class(self) -> torch.Tensor:
        """u[y] = Œ≤[y] / Œ±[y] - d√πng cho classifier."""
        u_group = self.beta_group / self.alpha_group.clamp(min=eps)
        return u_group[self.class_to_group]
    
    def predict(self, posterior: torch.Tensor):
        """h(x) = argmax_y u[y] * p_y(x) = argmax_y (Œ≤[y]/Œ±[y]) * p_y(x)"""
        u = self._u_class().unsqueeze(0)
        return (posterior * u).argmax(dim=-1)
```

#### 4.3.2. Exponentiated Gradient Update

```python
def search(self, plugin, posterior_s1, labels_s1, posterior_s2, labels_s2, ...):
    """Worst-group optimization v·ªõi exponentiated gradient."""
    
    # Kh·ªüi t·∫°o Œ≤ uniform
    num_groups = self.config.num_groups
    beta = np.ones(num_groups) / num_groups  # [0.5, 0.5] cho 2 groups
    
    best_result = None
    best_worst_error = float('inf')
    
    # Outer loop: Exponentiated gradient
    for t in range(self.num_outer_iters):
        # B∆∞·ªõc 1: Inner optimization v·ªõi Œ≤^(t)
        # G·ªçi Algorithm 1 v·ªõi weighted objective
        result = self.inner_optimizer.search(
            plugin, posterior_s1, labels_s1,
            beta=torch.tensor(beta, ...),  # Pass Œ≤ weights
            ...
        )
        
        # B∆∞·ªõc 2: Compute group errors tr√™n S2
        plugin.set_parameters(
            alpha=result.alpha,
            mu=result.mu,
            cost=result.cost
        )
        
        predictions_s2 = plugin.predict_class(posterior_s2)
        reject_s2 = plugin.predict_reject(posterior_s2)
        
        # T√≠nh group errors tr√™n accepted samples
        group_errors = []
        for g in range(num_groups):
            group_mask = class_to_group[labels_s2] == g
            accepted_in_group = group_mask & (~reject_s2)
            
            if accepted_in_group.sum() > 0:
                errors = (predictions_s2[accepted_in_group] != 
                         labels_s2[accepted_in_group]).sum()
                group_error = errors.float() / accepted_in_group.sum().float()
            else:
                group_error = 1.0  # Worst case
            
            group_errors.append(group_error.item())
        
        worst_error = max(group_errors)
        
        # Track best
        if worst_error < best_worst_error:
            best_worst_error = worst_error
            best_result = result
        
        # B∆∞·ªõc 3: Update Œ≤ b·∫±ng exponentiated gradient
        # Œ≤^(t+1)_k ‚àù Œ≤^(t)_k * exp(Œæ * e_k^(t))
        beta_old = beta.copy()
        beta = beta * np.exp(self.learning_rate * np.array(group_errors))
        
        # Normalize v·ªÅ simplex
        beta = beta / beta.sum()
        
        # Early stopping n·∫øu Œ≤ h·ªôi t·ª•
        if np.abs(beta - beta_old).max() < 1e-6:
            break
    
    return best_result
```

### 4.4. V√≠ D·ª• Workflow

**Iteration 0:**
- `Œ≤^(0) = [0.5, 0.5]` (uniform)
- Algorithm 1 t·ªëi ∆∞u weighted objective: `0.5 * e_head + 0.5 * e_tail`
- Group errors: `e_head = 0.15`, `e_tail = 0.40`
- Worst error: `max(0.15, 0.40) = 0.40`

**Iteration 1:**
- Update Œ≤: `Œ≤^(1) ‚àù [0.5, 0.5] * exp([0.15, 0.40]) = [0.58, 0.74]`
- Normalize: `Œ≤^(1) = [0.44, 0.56]` (tail ƒë∆∞·ª£c up-weight)
- Algorithm 1 t·ªëi ∆∞u v·ªõi `Œ≤^(1)`: t·∫≠p trung v√†o tail error
- Group errors: `e_head = 0.18`, `e_tail = 0.35`
- Worst error: `max(0.18, 0.35) = 0.35` ‚úì (t·ªët h∆°n!)

**Iteration 2:**
- Update Œ≤: `Œ≤^(2) ‚àù [0.44, 0.56] * exp([0.18, 0.35]) = [0.52, 0.78]`
- Normalize: `Œ≤^(2) = [0.40, 0.60]` (tail c√≤n ƒë∆∞·ª£c up-weight h∆°n)
- ...

**Convergence:**
- Œ≤ h·ªôi t·ª• v·ªÅ `[0.35, 0.65]` (tail ƒë∆∞·ª£c up-weight nhi·ªÅu h∆°n)
- Worst-group error gi·∫£m xu·ªëng `0.32`

---

## 5. So S√°nh Hai Modes

### 5.1. B·∫£ng So S√°nh

| Ti√™u ch√≠ | Balance Mode (Algorithm 1) | Worst Mode (Algorithm 2) |
|----------|---------------------------|-------------------------|
| **Objective** | Minimize balanced error: `(1/K) * Œ£_k e_k` | Minimize worst-group error: `max_k e_k` |
| **Parameters** | `Œ±` (coverage), `Œº` (threshold) | `Œ±`, `Œº`, `Œ≤` (group weights) |
| **Algorithm** | Power iteration tr√™n `Œ±` | Exponentiated gradient tr√™n `Œ≤` + Power iteration |
| **Focus** | C√¢n b·∫±ng error gi·ªØa c√°c groups | T·∫≠p trung v√†o group c√≥ error cao nh·∫•t |
| **Use Case** | Khi mu·ªën fair performance | Khi mu·ªën guarantee cho worst-case |

### 5.2. V√≠ D·ª• S·ªë

**Gi·∫£ s·ª≠:**
- Head error: 15%
- Tail error: 35%

**Balance Mode:**
- Balanced error = (15% + 35%) / 2 = **25%**
- C√≥ th·ªÉ hy sinh m·ªôt ch√∫t head error ƒë·ªÉ gi·∫£m tail error

**Worst Mode:**
- Worst-group error = max(15%, 35%) = **35%**
- T·∫≠p trung gi·∫£m tail error (35%) xu·ªëng, c√≥ th·ªÉ head error tƒÉng l√™n 18%
- K·∫øt qu·∫£: worst-group error = **32%** (t·ªët h∆°n!)

### 5.3. Khi N√†o D√πng Mode N√†o?

**D√πng Balance Mode khi:**
- Mu·ªën fair performance across all groups
- Ch·∫•p nh·∫≠n trade-off: m·ªôt s·ªë groups t·ªët h∆°n, m·ªôt s·ªë groups k√©m h∆°n
- ƒê√°nh gi√° b·∫±ng balanced error

**D√πng Worst Mode khi:**
- C·∫ßn guarantee cho worst-case scenario
- Kh√¥ng th·ªÉ ch·∫•p nh·∫≠n m·ªôt group c√≥ error qu√° cao
- ƒê√°nh gi√° b·∫±ng worst-group error (v√≠ d·ª•: fairness constraints)

---

## 6. T√≥m T·∫Øt

### 6.1. Balanced Error

- **C√¥ng th·ª©c**: `(1/K) * Œ£_k P(y ‚â† h(x) | r(x) = 0, y ‚àà G_k)`
- **T√≠nh ch·∫•t**: Trung b√¨nh c·ªßa conditional error rates tr√™n t·ª´ng group
- **Code location**: `compute_metrics()` trong c√°c file plugin

### 6.2. Balance Mode (Algorithm 1)

- **M·ª•c ti√™u**: Minimize balanced error
- **Algorithm**: Power iteration ƒë·ªÉ t√¨m optimal `Œ±`
- **Key idea**: Update `Œ±` d·ª±a tr√™n empirical coverage
- **Code**: `LtRPowerIterOptimizer` trong `src/models/ltr_plugin.py`

### 6.3. Worst Mode (Algorithm 2)

- **M·ª•c ti√™u**: Minimize worst-group error
- **Algorithm**: Exponentiated gradient tr√™n `Œ≤` + Power iteration tr√™n `Œ±`
- **Key idea**: Up-weight groups c√≥ error cao ƒë·ªÉ t·∫≠p trung optimize
- **Code**: `LtRWorstGroupOptimizer` trong `src/models/ltr_plugin.py`

---

## 7. References

- Paper: "Learning to Reject Meets Long-Tail Learning" (ICLR 2024)
- Code files:
  - `run_balanced_plugin_gating.py`
  - `run_worst_plugin_gating.py`
  - `src/models/ltr_plugin.py`
  - `src/metrics/reweighted_metrics.py`

